{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1TavRmAIUudUEWYVUCqQFNLnCpIoKAJDb","authorship_tag":"ABX9TyNvCdewI0TMf/zzxd5Rh+ar"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# GPT 모델을 원하는 자연어 데이터들을 이용해 사전학습"],"metadata":{"id":"OYh8_nL9BQp9","executionInfo":{"status":"ok","timestamp":1742263892174,"user_tz":-540,"elapsed":75,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w69XK4pEBFUJ","executionInfo":{"status":"ok","timestamp":1742263873445,"user_tz":-540,"elapsed":7169,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"0589cc15-b21a-4f5c-c74a-150e486f63b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["!mkdir my_data\n","!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Amu5v9lmBsKZ","executionInfo":{"status":"ok","timestamp":1742270469378,"user_tz":-540,"elapsed":5967,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"2a334393-ab31-4fb5-dfb6-051361d1555e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 1323k  100 1323k    0     0   516k      0  0:00:02  0:00:02 --:--:-- 1246k\n"]}]},{"cell_type":"code","source":["path = \"/content/my_data/wiki_20190620_small.txt\""],"metadata":{"id":"9uTlA_Y3BvKW","executionInfo":{"status":"ok","timestamp":1742270469406,"user_tz":-540,"elapsed":13,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# BERT 모델"],"metadata":{"id":"t8LW3BC0BnId"}},{"cell_type":"code","source":["from tokenizers import SentencePieceBPETokenizer\n","from tokenizers.normalizers import BertNormalizer\n","\n","tokenizer = SentencePieceBPETokenizer()\n","\n","tokenizer._tokenizer.normalizer = BertNormalizer(clean_text=True,\n","handle_chinese_chars=False,\n","lowercase=False)\n","\n","tokenizer.train(\n","    path,\n","    vocab_size=10000,\n","    special_tokens=[\n","        \"<s>\", # 문장의 시작\n","        \"<pad>\", # 패딩 토큰\n","        \"</s>\", # 문장의 끝\n","        \"<unk>\", # 사전에 없는 토큰\n","    ],\n",")"],"metadata":{"id":"1GmBd_3MBK0-","executionInfo":{"status":"ok","timestamp":1742270472087,"user_tz":-540,"elapsed":2054,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n","# SentencePiece를 사용하면, 나중에 decoding 과정에서 '_' 만 ' '로 replace해주면 띄어쓰기 복원이 가능해짐."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gkx-mrrVBpRn","executionInfo":{"status":"ok","timestamp":1742270472124,"user_tz":-540,"elapsed":32,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"b4a82928-30d4-4423-9928-2693fdec57e5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[1005, 578, 6613, 1303, 1041, 2071, 1136, 595, 1033]\n","['▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.']\n","이순신은 조선 중기의 무신이다.\n"]}]},{"cell_type":"code","source":["tokenizer.save_model('/content/drive/MyDrive/1자연어처리/sentencepiece_tokenizer')"],"metadata":{"id":"UIa3x4hyCEgM","executionInfo":{"status":"ok","timestamp":1742270507094,"user_tz":-540,"elapsed":4,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["tokenizer = SentencePieceBPETokenizer.from_file(vocab_filename='/content/drive/MyDrive/1자연어처리/sentencepiece_tokenizer/vocab.json',\n","                                                merges_filename='/content/drive/MyDrive/1자연어처리/sentencepiece_tokenizer/merges.txt')"],"metadata":{"id":"UP-KkH8FCfmw","executionInfo":{"status":"ok","timestamp":1742270509459,"user_tz":-540,"elapsed":3,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHNWgb9CDpv-","executionInfo":{"status":"ok","timestamp":1742270512669,"user_tz":-540,"elapsed":44,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"43751167-ba0a-4b79-beb0-f53e1e52334f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n","[1005, 579, 6613, 1303, 1041, 2071, 1136, 596, 1033]\n","['▁<', 's', '>', '이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.', '<', '/s', '>']\n","<s>이순신은 조선 중기의 무신이다.</s>\n"]}]},{"cell_type":"code","source":["tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<shkim>\"])\n","tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n","tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n","tokenizer.bos_token_id = tokenizer.token_to_id(\"<s>\")\n","tokenizer.eos_token_id = tokenizer.token_to_id(\"</s>\")\n","\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids)\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"En1Jc6orYKpQ","executionInfo":{"status":"ok","timestamp":1742270513412,"user_tz":-540,"elapsed":24,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"84c8790c-d1bf-4292-8593-1821d8f2e203"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1005, 579, 6613, 1303, 1041, 2071, 1136, 596, 1033, 2]\n","['<s>', '▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.', '</s>']\n","이순신은 조선 중기의 무신이다.\n"]}]},{"cell_type":"markdown","source":["# GPT-2 불러오기"],"metadata":{"id":"SUDVlwEMYlLo"}},{"cell_type":"code","source":["from transformers import GPT2Config, GPT2LMHeadModel\n","\n","config = GPT2Config(\n","  vocab_size=tokenizer.get_vocab_size(),\n","  bos_token_id=tokenizer.token_to_id(\"<s>\"),\n","  eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",")\n","\n","model = GPT2LMHeadModel(config)"],"metadata":{"id":"nnx8SwdWYdmq","executionInfo":{"status":"ok","timestamp":1742270532294,"user_tz":-540,"elapsed":17730,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model.num_parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ki9qod3xYppC","executionInfo":{"status":"ok","timestamp":1742270532324,"user_tz":-540,"elapsed":31,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"cb9b5c4f-cc88-487e-d3b9-a7ce7347f0dd"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["93523200"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import json\n","import os\n","import pickle\n","import random\n","import time\n","import warnings\n","from typing import Dict, List, Optional\n","\n","import torch\n","from torch.utils.data.dataset import Dataset\n","\n","from filelock import FileLock\n","\n","from transformers.tokenization_utils import PreTrainedTokenizer\n","from transformers.utils import logging"],"metadata":{"id":"O1c0KzaLYsqC","executionInfo":{"status":"ok","timestamp":1742270569359,"user_tz":-540,"elapsed":25,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["logger = logging.get_logger(__name__)\n","\n","class TextDataset(Dataset):\n","    \"\"\"\n","    This will be superseded by a framework-agnostic approach soon.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        tokenizer: PreTrainedTokenizer,\n","        file_path: str,\n","        block_size: int,\n","        overwrite_cache=False,\n","        cache_dir: Optional[str] = None,\n","    ):\n","        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n","\n","        block_size = block_size - tokenizer.num_special_tokens_to_add(is_pair=False)\n","\n","        directory, filename = os.path.split(file_path)\n","        cached_features_file = os.path.join(\n","            cache_dir if cache_dir is not None else directory,\n","            \"cached_lm_{}_{}_{}\".format(\n","                tokenizer.__class__.__name__,\n","                str(block_size),\n","                filename,\n","            ),\n","        )\n","\n","        # Make sure only the first process in distributed training processes the dataset,\n","        # and the others will use the cache.\n","        lock_path = cached_features_file + \".lock\"\n","        with FileLock(lock_path):\n","\n","            if os.path.exists(cached_features_file) and not overwrite_cache:\n","                start = time.time()\n","                with open(cached_features_file, \"rb\") as handle:\n","                    self.examples = pickle.load(handle)\n","                logger.info(\n","                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n","                )\n","\n","            else:\n","                logger.info(f\"Creating features from dataset file at {directory}\")\n","                # 여기서부터 본격적으로 데이터셋을 만들기 시작\n","                self.examples = []\n","                text = \"\"\n","                with open(file_path, encoding=\"utf-8\") as f:\n","                    lines = f.readlines()\n","                    for line in lines:\n","                        line = line.strip()\n","                        line = \"<s>\"+line+\"</s>\" # 학습 데이터 앞 뒤에 문장 구분 기호를 추가해\n","                        text += line    # 'text' 객체에 모든 학습 데이터를 다 합쳐버림\n","                tokenized_text = tokenizer.encode(text).ids\n","\n","                # 모델의 최대 sequence length만큼 데이터를 잘라서 저장\n","                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n","                    self.examples.append(\n","                        tokenized_text[i : i + block_size]\n","                    )\n","                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n","                # If your dataset is small, first you should look for a bigger one :-) and second you\n","                # can change this behavior by adding (model specific) padding.\n","\n","                start = time.time()\n","                with open(cached_features_file, \"wb\") as handle:\n","                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","                logger.info(\n","                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n","                )\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i) -> torch.Tensor:\n","        return torch.tensor(self.examples[i], dtype=torch.long)"],"metadata":{"id":"DYVZjFYjY0Ei","executionInfo":{"status":"ok","timestamp":1742270570865,"user_tz":-540,"elapsed":33,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path=path,\n","    block_size=128,\n",")\n","\n","from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(    # GPT는 생성모델이기 때문에 [MASK] 가 필요 없음\n","    tokenizer=tokenizer, mlm=False,)"],"metadata":{"id":"YUU3obkqZjpp","executionInfo":{"status":"ok","timestamp":1742270573264,"user_tz":-540,"elapsed":1173,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(dataset[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrUpbnjdZoBu","executionInfo":{"status":"ok","timestamp":1742270573972,"user_tz":-540,"elapsed":26,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"45e5e2cf-d350-49ee-807d-b0f4d38cefc0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([   0, 3997, 3546, 8404,  464,    5, 5481, 9525, 1798, 1890, 2297, 1262,\n","        9623, 2679, 1188, 2174,    2,    0, 5709, 5481,  256, 6466,  751, 3426,\n","         872, 1556,  681,  895, 1627, 9220,  588, 3621, 1010, 3303,    2,    0,\n","        6466, 7416, 2305,  404, 2217, 1074,    2,    0, 1013, 1107, 3716,  647,\n","        8574, 1024,  940,   94, 7321,  372,   94,  722, 9292,  706, 1651,  454,\n","        3166, 1032, 1074,    2,    0, 6343, 1262, 3716, 1009, 2932, 1176,  913,\n","        2037, 1171, 3227,  844,   94,  440,  974, 1486, 1017,    3, 1323, 3914,\n","        2095, 1042,    2,    0, 1382, 2068, 2225, 1095,  327,  844, 1823,  507,\n","           5, 1240, 7696,    2,    0, 3897, 6466, 1053, 1077,  687, 2318, 4649,\n","        5204, 5672, 1013, 1759,  116, 2742, 3004,  105,  656, 2283, 9762, 1192,\n","        1796, 2449, 2546, 9936, 6466, 1053, 1037,  534])\n"]}]},{"cell_type":"markdown","source":["# GPT-2 학습"],"metadata":{"id":"qnfO9PA9Zsp6"}},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='model_output',\n","    overwrite_output_dir=True,\n","    num_train_epochs=50,\n","    per_device_train_batch_size=64, # 512:32  # 128:64\n","    save_steps=1000,\n","    save_total_limit=2,\n","    logging_steps=100\n","\n",")"],"metadata":{"id":"zUirqf2RZqQB","executionInfo":{"status":"ok","timestamp":1742270577809,"user_tz":-540,"elapsed":1680,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")\n"],"metadata":{"id":"SmEXTd4EZ3BB","executionInfo":{"status":"ok","timestamp":1742270582834,"user_tz":-540,"elapsed":2875,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":817},"id":"dP1iipFGZ5i6","executionInfo":{"status":"ok","timestamp":1742272850898,"user_tz":-540,"elapsed":2266891,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"26592fdb-81b9-4491-f1d8-5a4ccf3f811d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjeongrak5\u001b[0m (\u001b[33mjeongrak5-not\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.8"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250318_040313-gwqllzjz</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/jeongrak5-not/huggingface/runs/gwqllzjz' target=\"_blank\">model_output</a></strong> to <a href='https://wandb.ai/jeongrak5-not/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/jeongrak5-not/huggingface' target=\"_blank\">https://wandb.ai/jeongrak5-not/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/jeongrak5-not/huggingface/runs/gwqllzjz' target=\"_blank\">https://wandb.ai/jeongrak5-not/huggingface/runs/gwqllzjz</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/1500 37:35, Epoch 50/50]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>7.733800</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>7.087100</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>6.625900</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>6.226300</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>5.892700</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>5.593400</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>5.318700</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>5.085300</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>4.879300</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>4.685800</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>4.535400</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>4.401300</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>4.293100</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>4.213600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>4.168900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1500, training_loss=5.382701314290364, metrics={'train_runtime': 2266.4257, 'train_samples_per_second': 41.85, 'train_steps_per_second': 0.662, 'total_flos': 6195887308800000.0, 'train_loss': 5.382701314290364, 'epoch': 50.0})"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["trainer.save_model('/content/drive/MyDrive/1자연어처리/gpt-2')"],"metadata":{"id":"WwAUKq0iZ7lB","executionInfo":{"status":"ok","timestamp":1742272852915,"user_tz":-540,"elapsed":2002,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["use_GPU = 1\n","\n","device = torch.device(\"cuda\" if (torch.cuda.is_available() and use_GPU) else \"cpu\")"],"metadata":{"id":"dm9uJMmha8ig","executionInfo":{"status":"ok","timestamp":1742272859824,"user_tz":-540,"elapsed":2,"user":{"displayName":"손정락","userId":"10892662232752168928"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# GPT-2 사용\n","\n","허깅페이스에서 제공해주는 GPT의 경우 generate라는 함수를 제공\n","\n","input_ids는 시작 토큰을 너어주고 모델에게 제공해주면 자동으로 뒷부분을 생성.\n","\n","generate의 설정에 따라 생성되는 방식이 변할 수 있음"],"metadata":{"id":"uvER0_0ObI4r"}},{"cell_type":"code","source":["import torch\n","torch.manual_seed(42)\n","\n","# Device configuration\n","device = torch.device('cuda' if (torch.cuda.is_available() and use_GPU) else 'cpu')\n","\n","input_ids = torch.tensor(tokenizer.encode(\"<s>이순신\", add_special_tokens=True).ids).unsqueeze(0).to('cuda')\n","\n","output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)\n","for generated_sequence in output_sequences:\n","    generated_sequence = generated_sequence.tolist()\n","    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, skip_special_tokens=True)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cak62FtTa1lT","executionInfo":{"status":"ok","timestamp":1742272873364,"user_tz":-540,"elapsed":2871,"user":{"displayName":"손정락","userId":"10892662232752168928"}},"outputId":"e22a783a-232d-4ef8-888d-c3dc818cfaf1"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["GENERATED SEQUENCE : 이순신 아도 대위전상을 했지만 고졸에서 연영노는 그러나 \"유로 모시한 동시 대작부터 농우상장 김수산운 km2경우절·북스나지만 터키시라들 중중중대하지 못변치에서는 위키가동기수전경정·국군들이 소 사건구위상사 등을 진행하면서 편시 대통령직대 총선에서 부전·파 등을 통해 일간 휴 박\n","GENERATED SEQUENCE : 이순신할 수 있는 나다.\n","GENERATED SEQUENCE : 이순신치, 김의 한국, 사회량, 영동과 기·구동 등을 멸망력이 큰 영향을 끼 점령 등과 등의 다양한 통일질들의 제안했다.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6nYcPAPfjhJC"},"execution_count":null,"outputs":[]}]}